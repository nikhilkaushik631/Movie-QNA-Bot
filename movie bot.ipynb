{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community langchain-google-genai langgraph langchain-groq tavily-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langgraph.graph import StateGraph, END\n",
    "from google.colab import userdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MovieQueryState(BaseModel):\n",
    "    \"\"\"State for the movie query processing system\"\"\"\n",
    "    original_query: str = Field(description=\"Original user query\")\n",
    "    raw_search_results: List[Dict] = Field(default_factory=list, description=\"Raw search results\")\n",
    "    cleaned_content: List[str] = Field(default_factory=list, description=\"Cleaned relevant paragraphs\")\n",
    "    final_response: str = Field(default=\"\", description=\"Final synthesized response\")\n",
    "    sources: List[Dict] = Field(default_factory=list, description=\"Sources used\")\n",
    "    chat_history: List[str] = Field(default_factory=list, description=\"Chat conversation history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_llm = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\") \n",
    "\n",
    "tavily_search = TavilySearchAPIWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa49836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_agent(state: MovieQueryState) -> MovieQueryState:\n",
    "    \"\"\"Performs web searches based on the query\"\"\"\n",
    "    search_results = []\n",
    "\n",
    "    # Directly perform search for the query (no query analysis)\n",
    "    results = tavily_search.results(state.original_query, max_results=5)  # Limit to top 5 results\n",
    "    for result in results:\n",
    "        result[\"query\"] = state.original_query\n",
    "        result[\"timestamp\"] = datetime.now().isoformat()\n",
    "        search_results.append(result)\n",
    "\n",
    "    state.raw_search_results = search_results\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a Content Extraction Specialist. Your job is to extract the most relevant information from search results. extract the content on movies and tv series.\n",
    "\n",
    "Search results:\n",
    "{raw_search_results}\n",
    "\n",
    "Extract the most relevant 1-2 paragraphs per result, focusing on clarity and directness.\n",
    "\n",
    "Response:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_extraction_agent(state: MovieQueryState) -> MovieQueryState:\n",
    "    \"\"\"Extracts relevant paragraphs from search results\"\"\"\n",
    "    chain = LLMChain(llm=gemini_llm, prompt=content_extraction_prompt)\n",
    "    response = chain.invoke({\n",
    "        \"original_query\": state.original_query,      # ← add this\n",
    "        \"raw_search_results\": state.raw_search_results\n",
    "    })\n",
    "    lines = response['text'].strip().splitlines()\n",
    "    state.cleaned_content = [line.strip() for line in lines if line.strip()]\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a Movie Information Specialist. Your job is to synthesize a helpful, accurate response.\n",
    "\n",
    "Based on the content below, write a comprehensive, helpful response to the original query.\n",
    "The response should be in a paragraph do not use points.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Original Query: {original_query}\n",
    "\n",
    "Content:\n",
    "{cleaned_content}\n",
    "\n",
    "Guidelines:\n",
    "- For factual: Be direct and precise.\n",
    "- For plot: Narrate like a storyteller.\n",
    "- For opinion: Present balanced views.\n",
    "- For analytical: Provide deep insights.\n",
    " \n",
    "\n",
    "Response:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f774778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_synthesis_agent(state: MovieQueryState) -> MovieQueryState:\n",
    "    \"\"\"Synthesizes the final response\"\"\"\n",
    "    chain = LLMChain(llm=llama3_llm, prompt=response_synthesis_prompt)\n",
    "    response = chain.invoke({\n",
    "        \"original_query\": state.original_query,\n",
    "        \"cleaned_content\": \"\\n\".join(state.cleaned_content)\n",
    "        \"chat_history\": \"\\n\".join(state.chat_history) if state.chat_history else \"None\"\n",
    "    })\n",
    "\n",
    "    state.final_response = response['text'].strip()\n",
    "\n",
    "    sources = []\n",
    "    for item in state.raw_search_results:\n",
    "        sources.append({\n",
    "            \"url\": item[\"url\"],\n",
    "            \"title\": item.get(\"title\", \"Unknown\"),\n",
    "            \"published_date\": item.get(\"published\", \"Unknown\")\n",
    "        })\n",
    "\n",
    "    state.sources = sources\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "movibot = StateGraph(MovieQueryState)\n",
    "\n",
    "\n",
    "movibot.add_node(\"web_search\", web_search_agent)\n",
    "movibot.add_node(\"content_extraction\", content_extraction_agent)\n",
    "movibot.add_node(\"response_synthesis\", response_synthesis_agent)\n",
    "\n",
    "movibot.add_edge(\"web_search\", \"content_extraction\")\n",
    "movibot.add_edge(\"content_extraction\", \"response_synthesis\")\n",
    "movibot.add_edge(\"response_synthesis\", END)\n",
    "\n",
    "movibot.set_entry_point(\"web_search\")\n",
    "\n",
    "movie_companion_agent = movibot.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_followup_query(query: str, chat_history: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Rewrites a followup query by incorporating context from the chat history,\n",
    "    so it becomes self-contained.\n",
    "    \"\"\"\n",
    "    rewriting_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "You are a helpful query rewriting agent. Given the conversation history and the new followup query, \n",
    "reformulate the query so that it is self-contained and includes all necessary context to be answered properly.\n",
    "\n",
    "Conversation History:\n",
    "{chat_history}\n",
    "\n",
    "Followup Query:\n",
    "{query}\n",
    "\n",
    "Self-Contained Query:\"\"\")\n",
    "    chain = LLMChain(llm=llama3_llm, prompt=rewriting_prompt)\n",
    "    response = chain.invoke({\n",
    "        \"chat_history\": \"\\n\".join(chat_history),\n",
    "        \"query\": query\n",
    "    })\n",
    "    return response[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_information(query: str, chat_history: List[str] = []) -> Dict[str, Any]:\n",
    "    # If there is prior chat history, rewrite the query to include context.\n",
    "    if chat_history:\n",
    "        query = rewrite_followup_query(query, chat_history)\n",
    "    \n",
    "    initial_state = MovieQueryState(original_query=query, chat_history=chat_history)\n",
    "    result_values = movie_companion_agent.invoke(initial_state)\n",
    "    \n",
    "    return {\n",
    "        \"response\": result_values[\"final_response\"],\n",
    "        \"sources\": result_values[\"sources\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_query_with_llm(query: str) -> bool:\n",
    "    \"\"\"Uses an LLM to check if the query is related to movies or TV series.\"\"\"\n",
    "    validation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a Movie Query Validator. Determine if the following query is related to movies or TV series. \n",
    "It can be a question, a request for information, or a general inquiry.\n",
    "It can contain titles, actors, directors, or any other relevant details.\n",
    "Return \"yes\" if it is, or \"no\" if it is not, with no extra commentary.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer:\"\"\")\n",
    "    chain = LLMChain(llm=gemini_llm, prompt=validation_prompt)\n",
    "    response = chain.invoke({\"query\": query})\n",
    "    answer = response['text'].strip().lower()\n",
    "    return answer.startswith(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Simple CLI interface\n",
    "    chat_count = 3\n",
    "    print(\"Movie Companion System\")\n",
    "    print(\"-----------------------\")\n",
    "    chat_memory = [] \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your movie question (or 'q' to quit, 'clear' to reset conversation): \")\n",
    "        \n",
    "        if query.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        if query.lower() == 'clear':\n",
    "            chat_memory = []\n",
    "            print(\"Chat history cleared.\")\n",
    "            continue\n",
    "\n",
    "        if not validate_query_with_llm(query):\n",
    "            print(\"The query does not appear to be related to movies or TV series. Please ask a relevant question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nProcessing your query...\")\n",
    "        chat_memory.append(f\"User: {query}\")\n",
    "        if len(chat_memory) > chat_count:\n",
    "            chat_memory = chat_memory[-chat_count:]\n",
    "        \n",
    "        try:\n",
    "            result = get_movie_information(query, chat_history=chat_memory)\n",
    "            \n",
    "            bot_response = result['response']\n",
    "            chat_memory.append(f\"Bot: {bot_response}\")\n",
    "            \n",
    "            print(\"\\nRESPONSE:\")\n",
    "            print(f\"A: {bot_response}\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
